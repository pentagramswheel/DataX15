{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# Homework 09\n",
    "Sentiment Analysis on IMDB movie reviews\n",
    "\n",
    "Reference: https://github.com/ikhlaqsidhu/data-x/blob/master/07-tools-webscraping-crawling-nlp-sentiment-sc1t/notebook-nlp-sentiment-analysis-imdb-afo_v1.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# As you go through the notebook, you will encounter these main steps in the code: \n",
    "1. Reading of file labeledTrainData.tsv from data folder in a dataframe `train`.\n",
    "2. A function review_cleaner(train['review'],lemmatize,stem) which cleans the reviews in the input file.\n",
    "3. A function train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000\n",
    "4. You will see a model has been trained on unigrams of the reviews without lemmatizing and stemming.\n",
    "5. Your task is in 5.TODO section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "## Data set\n",
    "\n",
    "The labeled training data set consists of 25,000 IMDB movie reviews. There is also an unlabeled test set with 25,000 IMDB movie reviews. The sentiment of the reviews are binary, meaning an IMDB rating < 5 results in a sentiment score of 0, and a rating >=7 have a sentiment score of 1 (no reviews with score 5 or 6 are included in the analysis). No individual movie has more than 30 reviews.\n",
    "\n",
    "## File description\n",
    "\n",
    "* **labeledTrainData** - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review. \n",
    "\n",
    "* **testData** - The unlabeled test set. 25,000 rows containing an id, and text for each review. \n",
    "\n",
    "## Data columns\n",
    "* **id** - Unique ID of each review\n",
    "* **sentiment** - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* **review** - Text of the review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 1. Data set statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd       \n",
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "# train.shape should be (25000,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div id='sec3'></div>\n",
    "##  2.Preparing the data set for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll create a function called `review_cleaner` that reads in a review and:\n",
    "\n",
    "- Removes HTML tags (using beautifulsoup)\n",
    "- **Extract emoticons (emotion symbols, aka smileys :D )**\n",
    "- Removes non-letters (using regular expression)\n",
    "- Converts all words to lowercase letters and tokenizes them (using .split() method on the review strings, so that every word in the review is an element in a list)\n",
    "- Removes all the English stopwords from the list of movie review words\n",
    "- Join the words back into one string seperated by space, append the emoticons to the end\n",
    "\n",
    "**NOTE: Transform the list of stopwords to a set before removing the stopwords. I.e. assign `eng_stopwords = set(stopwords.words(\"english\"))`. Use the set to look up stopwords. This will speed up the computations A LOT (Python is much quicker when searching a set than a list).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def review_cleaner(reviews,lemmatize=True,stem=False):\n",
    "    '''\n",
    "    Clean and preprocess a review.\n",
    "\n",
    "    1. Remove HTML tags\n",
    "    2. Use regex to remove all special characters (only keep letters)\n",
    "    3. Make strings to lower case and tokenize / word split reviews\n",
    "    4. Remove English stopwords\n",
    "    5. Rejoin to one string\n",
    "    '''\n",
    "    ps = PorterStemmer()\n",
    "    wnl = WordNetLemmatizer()\n",
    "        #1. Remove HTML tags\n",
    "    \n",
    "    cleaned_reviews=[]\n",
    "    for i,review in enumerate(train['review']):\n",
    "    # print progress\n",
    "        if( (i+1)%500 == 0 ):\n",
    "            print(\"Done with %d reviews\" %(i+1))\n",
    "        review = bs.BeautifulSoup(review).text\n",
    "\n",
    "        #2. Use regex to find emoticons\n",
    "        emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', review)\n",
    "\n",
    "        #3. Remove punctuation\n",
    "        review = re.sub(\"[^a-zA-Z]\", \" \",review)\n",
    "\n",
    "        #4. Tokenize into words (all lower case)\n",
    "        review = review.lower().split()\n",
    "\n",
    "        #5. Remove stopwords\n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "            \n",
    "        clean_review=[]\n",
    "        for word in review:\n",
    "            if word not in eng_stopwords:\n",
    "                if lemmatize is True:\n",
    "                    word=wnl.lemmatize(word)\n",
    "                elif stem is True:\n",
    "                    if word == 'oed':\n",
    "                        continue\n",
    "                    word=ps.stem(word)\n",
    "                clean_review.append(word)\n",
    "\n",
    "        #6. Join the review to one sentence\n",
    "        \n",
    "        review_processed = ' '.join(clean_review+emoticons)\n",
    "        cleaned_reviews.append(review_processed)\n",
    "    \n",
    "\n",
    "    return(cleaned_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "##  3. Function to train and validate a sentiment analysis model using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# # CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def train_predict_sentiment(cleaned_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000):\n",
    "    '''This function will:\n",
    "    1. split data into train and test set.\n",
    "    2. get n-gram counts from cleaned reviews \n",
    "    3. train a random forest model using train n-gram counts and y (labels)\n",
    "    4. test the model on your test split\n",
    "    5. print accuracy of sentiment prediction on test and training data\n",
    "    6. print confusion matrix on test data results\n",
    "\n",
    "    To change n-gram type, set value of ngram argument\n",
    "    To change the number of features you want the countvectorizer to generate, set the value of max_features argument'''\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, ngram),analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = max_features) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "    #print('TOP 20 FEATURES ARE: ',(vectorizer.get_feature_names()[:20]))\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n",
    "\n",
    "    #Extract feature importnace\n",
    "    print('\\nTOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:20]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 4. Train and test  Model on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Done with 1000 reviews\n",
      "Done with 1500 reviews\n",
      "Done with 2000 reviews\n",
      "Done with 2500 reviews\n",
      "Done with 3000 reviews\n",
      "Done with 3500 reviews\n",
      "Done with 4000 reviews\n",
      "Done with 4500 reviews\n",
      "Done with 5000 reviews\n",
      "Done with 5500 reviews\n",
      "Done with 6000 reviews\n",
      "Done with 6500 reviews\n",
      "Done with 7000 reviews\n",
      "Done with 7500 reviews\n",
      "Done with 8000 reviews\n",
      "Done with 8500 reviews\n",
      "Done with 9000 reviews\n",
      "Done with 9500 reviews\n",
      "Done with 10000 reviews\n",
      "Done with 10500 reviews\n",
      "Done with 11000 reviews\n",
      "Done with 11500 reviews\n",
      "Done with 12000 reviews\n",
      "Done with 12500 reviews\n",
      "Done with 13000 reviews\n",
      "Done with 13500 reviews\n",
      "Done with 14000 reviews\n",
      "Done with 14500 reviews\n",
      "Done with 15000 reviews\n",
      "Done with 15500 reviews\n",
      "Done with 16000 reviews\n",
      "Done with 16500 reviews\n",
      "Done with 17000 reviews\n",
      "Done with 17500 reviews\n",
      "Done with 18000 reviews\n",
      "Done with 18500 reviews\n",
      "Done with 19000 reviews\n",
      "Done with 19500 reviews\n",
      "Done with 20000 reviews\n",
      "Done with 20500 reviews\n",
      "Done with 21000 reviews\n",
      "Done with 21500 reviews\n",
      "Done with 22000 reviews\n",
      "Done with 22500 reviews\n",
      "Done with 23000 reviews\n",
      "Done with 23500 reviews\n",
      "Done with 24000 reviews\n",
      "Done with 24500 reviews\n",
      "Done with 25000 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9999 \n",
      " The validation accuracy is:  0.8216\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2102  446]\n",
      "     pos   [ 446 2006]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'waste', 'awful', 'excellent', 'terrible', 'best', 'boring', 'worse']\n"
     ]
    }
   ],
   "source": [
    "#Clean the reviews in the training set 'train' using review_cleaner function defined above\n",
    "# Here we use the original reviews without lemmatizing and stemming\n",
    "\n",
    "original_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=False)\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.99995 \n",
      " The validation accuracy is:  0.8208\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [2121  427]\n",
      "     pos   [ 469 1983]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['bad', 'worst', 'great', 'awful', 'excellent', 'waste', 'terrible', 'wonderful', 'boring', 'best', 'nothing', 'worse', 'poor', 'love', 'minutes', 'horrible', 'stupid', 'waste time', 'movie', 'even']\n"
     ]
    }
   ],
   "source": [
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"],ngram=2,max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 5. TODO: \n",
    "To do this exercise you only need to change argument values in the functions review_cleaner and train_predict_semtiment.\n",
    "\n",
    "1. In __UNIGRAM setting__ ie. when ngram=1 in the function `train_predict_sentiment()`.Compare the performance of original cleaned reviews in Sentiment anlysis to \n",
    "    1. lemmatized reviews \n",
    "    2. stemmed reviews\n",
    "2. In __BIGRAM setting__ ie. when ngram=2 in the function `train_predict_sentiment()`.Compare the performance of original cleaned reviews in sentiment analysis to:\n",
    "     1. lemmatized reviews\n",
    "     2. stemmed reviews\n",
    "3. In __UNIGRAM setting__  and _lemmatize=True__ ie. when ngram=1, compare the performance of Sentiment analysis for these values of maximum features=[10,100,1000,5000], you can change the value of argument max_features in `train_predict_sentiment()\n",
    "    \n",
    "### SUBMISSION:  For each question in 5. TODO  report your results in a PDF. Write a 100-200 word summary of your observations overall.\n",
    "### Mention the  review_cleaner( ) and train_predict_sentiment( ) argument setting that you used in each case. Do not submit any ipython notebook.\n",
    "\n",
    "\n",
    "\n",
    "Eg: For original review with unigram and 5000 max_features, I will report:\n",
    "\n",
    "original_clean_reviews=review_cleaner(train['review'],lemmatize=False,stem=False)\n",
    "train_predict_sentiment(cleaned_reviews=original_clean_reviews, y=train[\"sentiment\"],ngram=1,max_features=5000)\n",
    "\n",
    "The training accuracy is:  1.0 \n",
    "The validation accuracy is:  0.836\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** Wil Aquino\n",
    "\n",
    "**Date:** October 28, 2021\n",
    "\n",
    "**Project:** Feature Engineering & Text Representation\n",
    "\n",
    "**Note:** This work is based off of Module 510 from Data X at Berkeley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Wil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import bs4 as bs\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# nltk.download('all')\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "# NOTE: The only Jupyter cells ran above are the cells with \n",
    "#       review_cleaner() and train_predict_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>employee_title</th>\n",
       "      <th>employee_status</th>\n",
       "      <th>review_title</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-15</td>\n",
       "      <td>Operator</td>\n",
       "      <td>Current Employee</td>\n",
       "      <td>Great place to work</td>\n",
       "      <td>Because of the work culture\\n</td>\n",
       "      <td>Swing shift and extended work hours\\nBe the fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-11</td>\n",
       "      <td>Process Engineer</td>\n",
       "      <td>Current Employee, more than 8 years</td>\n",
       "      <td>Great Compnay</td>\n",
       "      <td>Inclusive and diverse company! Great work and ...</td>\n",
       "      <td>I don't have nothing to share.\\nBe the first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-10</td>\n",
       "      <td>Team Member</td>\n",
       "      <td>Former Employee, less than 1 year</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Great job, excellent crew and perks. I miss it.\\n</td>\n",
       "      <td>It was the third best job I ever had.\\nVerify ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>Reliability Engineer</td>\n",
       "      <td>Current Employee</td>\n",
       "      <td>Great Place to Work</td>\n",
       "      <td>Excellent work life balance Very diverse and i...</td>\n",
       "      <td>Promotions can be slow moving\\nVerify your ema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>Sales Associate/Cashier</td>\n",
       "      <td>Current Employee</td>\n",
       "      <td>It’s cool Ig</td>\n",
       "      <td>Good pay Time off Good customers I don’t know ...</td>\n",
       "      <td>Bad managers Horrible coworkers Dirty all the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           employee_title                      employee_status  \\\n",
       "0  2021-10-15                 Operator                     Current Employee   \n",
       "1  2021-10-11         Process Engineer  Current Employee, more than 8 years   \n",
       "2  2021-10-10              Team Member    Former Employee, less than 1 year   \n",
       "3  2021-10-06     Reliability Engineer                     Current Employee   \n",
       "4  2021-10-06  Sales Associate/Cashier                     Current Employee   \n",
       "\n",
       "          review_title                                               pros  \\\n",
       "0  Great place to work                      Because of the work culture\\n   \n",
       "1        Great Compnay  Inclusive and diverse company! Great work and ...   \n",
       "2            Excellent  Great job, excellent crew and perks. I miss it.\\n   \n",
       "3  Great Place to Work  Excellent work life balance Very diverse and i...   \n",
       "4         It’s cool Ig  Good pay Time off Good customers I don’t know ...   \n",
       "\n",
       "                                                cons  \n",
       "0  Swing shift and extended work hours\\nBe the fi...  \n",
       "1  I don't have nothing to share.\\nBe the first t...  \n",
       "2  It was the third best job I ever had.\\nVerify ...  \n",
       "3  Promotions can be slow moving\\nVerify your ema...  \n",
       "4  Bad managers Horrible coworkers Dirty all the ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a part of our project data\n",
    "reviews = pd.read_csv(\"chevron_reviews.csv\", header = 0, sep = \";\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_title</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great place to work</td>\n",
       "      <td>Because of the work culture\\n</td>\n",
       "      <td>Swing shift and extended work hours\\nBe the fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great Compnay</td>\n",
       "      <td>Inclusive and diverse company! Great work and ...</td>\n",
       "      <td>I don't have nothing to share.\\nBe the first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>Great job, excellent crew and perks. I miss it.\\n</td>\n",
       "      <td>It was the third best job I ever had.\\nVerify ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great Place to Work</td>\n",
       "      <td>Excellent work life balance Very diverse and i...</td>\n",
       "      <td>Promotions can be slow moving\\nVerify your ema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s cool Ig</td>\n",
       "      <td>Good pay Time off Good customers I don’t know ...</td>\n",
       "      <td>Bad managers Horrible coworkers Dirty all the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review_title                                               pros  \\\n",
       "0  Great place to work                      Because of the work culture\\n   \n",
       "1        Great Compnay  Inclusive and diverse company! Great work and ...   \n",
       "2            Excellent  Great job, excellent crew and perks. I miss it.\\n   \n",
       "3  Great Place to Work  Excellent work life balance Very diverse and i...   \n",
       "4         It’s cool Ig  Good pay Time off Good customers I don’t know ...   \n",
       "\n",
       "                                                cons  \n",
       "0  Swing shift and extended work hours\\nBe the fi...  \n",
       "1  I don't have nothing to share.\\nBe the first t...  \n",
       "2  It was the third best job I ever had.\\nVerify ...  \n",
       "3  Promotions can be slow moving\\nVerify your ema...  \n",
       "4  Bad managers Horrible coworkers Dirty all the ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelim_clean_reviews = reviews.drop([\"date\", \"employee_title\", \"employee_status\"], axis = 1)\n",
    "prelim_clean_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_title</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great place to work</td>\n",
       "      <td>{ Pros. Because of the work culture } { Cons. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great Compnay</td>\n",
       "      <td>{ Pros. Inclusive and diverse company! Great w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>{ Pros. Great job, excellent crew and perks. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great Place to Work</td>\n",
       "      <td>{ Pros. Excellent work life balance Very diver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s cool Ig</td>\n",
       "      <td>{ Pros. Good pay Time off Good customers I don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>Mixed Bag</td>\n",
       "      <td>{ Pros. - Pay and benefits - Safety culture (i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>Customer Service Representative</td>\n",
       "      <td>{ Pros. Fast pace job, work is not hard,deal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>Senior Advisor</td>\n",
       "      <td>{ Pros. Work-life is well balanced; Relatively...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Administrative Assistant</td>\n",
       "      <td>{ Pros. Great health benefits and good compens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>The Titanic.</td>\n",
       "      <td>{ Pros. If you are a white engineer you should...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        review_title  \\\n",
       "0                Great place to work   \n",
       "1                      Great Compnay   \n",
       "2                          Excellent   \n",
       "3                Great Place to Work   \n",
       "4                       It’s cool Ig   \n",
       "..                               ...   \n",
       "984                        Mixed Bag   \n",
       "985  Customer Service Representative   \n",
       "986                   Senior Advisor   \n",
       "987         Administrative Assistant   \n",
       "988                     The Titanic.   \n",
       "\n",
       "                                                review  \n",
       "0    { Pros. Because of the work culture } { Cons. ...  \n",
       "1    { Pros. Inclusive and diverse company! Great w...  \n",
       "2    { Pros. Great job, excellent crew and perks. I...  \n",
       "3    { Pros. Excellent work life balance Very diver...  \n",
       "4    { Pros. Good pay Time off Good customers I don...  \n",
       "..                                                 ...  \n",
       "984  { Pros. - Pay and benefits - Safety culture (i...  \n",
       "985  { Pros. Fast pace job, work is not hard,deal w...  \n",
       "986  { Pros. Work-life is well balanced; Relatively...  \n",
       "987  { Pros. Great health benefits and good compens...  \n",
       "988  { Pros. If you are a white engineer you should...  \n",
       "\n",
       "[989 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove newline characters\n",
    "prelim_clean_reviews['pros'] = prelim_clean_reviews['pros'].str.replace('\\n', '', regex = True)\n",
    "prelim_clean_reviews['cons'] = prelim_clean_reviews['cons'].str.replace('\\n', '', regex = True)\n",
    "\n",
    "# remove bad strings caught by the web scraper\n",
    "badString1 = \"Verify your email to continue reading or Resend email\"\n",
    "badString2 = \"Be the first to find this review helpfulHelpfulShareRepor\"\n",
    "\n",
    "prelim_clean_reviews['cons'] = prelim_clean_reviews['cons'].str.replace(badString1, '', regex = True)\n",
    "prelim_clean_reviews['cons'] = prelim_clean_reviews['cons'].str.replace(badString2, '', regex = True)\n",
    "\n",
    "# compile pros and cons together into a column\n",
    "prelim_clean_reviews[\"review\"] = \"{ Pros. \" + prelim_clean_reviews['pros'] + \" } { Cons. \" + prelim_clean_reviews['cons'] + \" }\"\n",
    "clean_reviews = prelim_clean_reviews.drop([\"pros\", \"cons\"], axis = 1)\n",
    "\n",
    "# remove empty reviews\n",
    "clean_reviews = clean_reviews.dropna(subset = ['review'])\n",
    "clean_reviews = clean_reviews.reset_index(drop = True)\n",
    "\n",
    "clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{ Pros. Because of the work culture } { Cons. ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{ Pros. Inclusive and diverse company! Great w...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{ Pros. Great job, excellent crew and perks. I...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{ Pros. Excellent work life balance Very diver...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{ Pros. Good pay Time off Good customers I don...</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text negative neutral  \\\n",
       "0  { Pros. Because of the work culture } { Cons. ...      0.0     1.0   \n",
       "1  { Pros. Inclusive and diverse company! Great w...     0.07    0.75   \n",
       "2  { Pros. Great job, excellent crew and perks. I...     0.05    0.59   \n",
       "3  { Pros. Excellent work life balance Very diver...      0.0    0.73   \n",
       "4  { Pros. Good pay Time off Good customers I don...     0.31    0.53   \n",
       "\n",
       "  positive compound  \n",
       "0      0.0      0.0  \n",
       "1     0.17     0.54  \n",
       "2     0.36     0.91  \n",
       "3     0.27     0.86  \n",
       "4     0.16    -0.67  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve basic sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def round_two_digit_string(x):\n",
    "    return str(round(x, 2))\n",
    "\n",
    "def retrieve_sentiment_analysis(df, column):\n",
    "    \"\"\" Performs a sentiment analysis on the given data.\n",
    "    \n",
    "    Args:\n",
    "        df::[pd.DataFrame]\n",
    "            The table of given reviews and their statistics.\n",
    "            \n",
    "    Return:\n",
    "        sentiment_df::[pd.DataFrame]\n",
    "            The reviews with their sentiment analysis data.\n",
    "    \"\"\"\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    data = {\n",
    "        \"text\": [],\n",
    "        \"negative\": [],\n",
    "        \"neutral\": [],\n",
    "        \"positive\": [],\n",
    "        \"compound\": []\n",
    "    }\n",
    "\n",
    "    for sentence in df[column]:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "\n",
    "        data[\"text\"].append(sentence)\n",
    "        data[\"negative\"].append(round_two_digit_string(vs[\"neg\"]))\n",
    "        data[\"neutral\"].append(round_two_digit_string(vs[\"neu\"]))\n",
    "        data[\"positive\"].append(round_two_digit_string(vs[\"pos\"]))\n",
    "        data[\"compound\"].append(round_two_digit_string(vs[\"compound\"]))\n",
    "        \n",
    "    sentiment_df = pd.DataFrame(data)\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "sentiment_analysis = retrieve_sentiment_analysis(clean_reviews, \"review\")\n",
    "sentiment_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_title</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great place to work</td>\n",
       "      <td>{ Pros. Because of the work culture } { Cons. ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great Compnay</td>\n",
       "      <td>{ Pros. Inclusive and diverse company! Great w...</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>{ Pros. Great job, excellent crew and perks. I...</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great Place to Work</td>\n",
       "      <td>{ Pros. Excellent work life balance Very diver...</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s cool Ig</td>\n",
       "      <td>{ Pros. Good pay Time off Good customers I don...</td>\n",
       "      <td>-0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review_title                                             review  \\\n",
       "0  Great place to work  { Pros. Because of the work culture } { Cons. ...   \n",
       "1        Great Compnay  { Pros. Inclusive and diverse company! Great w...   \n",
       "2            Excellent  { Pros. Great job, excellent crew and perks. I...   \n",
       "3  Great Place to Work  { Pros. Excellent work life balance Very diver...   \n",
       "4         It’s cool Ig  { Pros. Good pay Time off Good customers I don...   \n",
       "\n",
       "  sentiment  \n",
       "0       0.0  \n",
       "1      0.54  \n",
       "2      0.91  \n",
       "3      0.86  \n",
       "4     -0.67  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_reviews[\"sentiment\"] = sentiment_analysis[\"compound\"]\n",
    "clean_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9987357774968394 \n",
      " The validation accuracy is:  0.09595959595959595\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'great', 'company', 'good', 'people', 'culture', 'pay', 'management', 'benefits', 'opportunities', 'job', 'employees', 'time', 'balance', 'life', 'get', 'hours', 'slow', 'salary', 'environment']\n"
     ]
    }
   ],
   "source": [
    "# original test\n",
    "train = clean_reviews\n",
    "original_clean_reviews = review_cleaner(train[\"review\"], lemmatize = False, stem = False)\n",
    "train_predict_sentiment(cleaned_reviews = original_clean_reviews, y = train[\"sentiment\"], ngram = 1, max_features = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Unigram Lemmatized Reviews\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9987357774968394 \n",
      " The validation accuracy is:  0.050505050505050504\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'good', 'company', 'people', 'great', 'pay', 'culture', 'employee', 'management', 'job', 'benefit', 'time', 'opportunity', 'lot', 'life', 'salary', 'balance', 'hour', 'environment', 'oil']\n",
      "----\n",
      "\n",
      "Q1 Unigram Stemmed Reviews\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.08080808080808081\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'good', 'compani', 'great', 'peopl', 'manag', 'pay', 'cultur', 'job', 'employe', 'benefit', 'time', 'opportun', 'lot', 'life', 'balanc', 'salari', 'hour', 'environ', 'oil']\n",
      "----\n",
      "\n",
      "Q2 Bigram Lemmatized Reviews\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9987357774968394 \n",
      " The validation accuracy is:  0.0707070707070707\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'company', 'people', 'good', 'great', 'culture', 'job', 'management', 'pay', 'employee', 'benefit', 'opportunity', 'time', 'lot', 'salary', 'hour', 'career', 'environment', 'oil', 'many']\n",
      "----\n",
      "\n",
      "Q2 Bigram Stemmed Reviews\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9987357774968394 \n",
      " The validation accuracy is:  0.11616161616161616\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'compani', 'good', 'peopl', 'manag', 'great', 'cultur', 'pay', 'opportun', 'job', 'employe', 'benefit', 'time', 'lot', 'get', 'salari', 'environ', 'schedul', 'oil', 'hour']\n",
      "----\n",
      "\n",
      "Q3 Unigram Reviews with 10 Features\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.44879898862199746 \n",
      " The validation accuracy is:  0.07575757575757576\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'company', 'people', 'good', 'employee', 'pay', 'great', 'benefit', 'con', 'pro']\n",
      "----\n",
      "\n",
      "Q3 Unigram Reviews with 100 Features\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9987357774968394 \n",
      " The validation accuracy is:  0.06565656565656566\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'company', 'good', 'people', 'great', 'pay', 'benefit', 'job', 'employee', 'culture', 'lot', 'management', 'opportunity', 'time', 'balance', 'environment', 'salary', 'get', 'career', 'life']\n",
      "----\n",
      "\n",
      "Q3 Unigram Reviews with 1000 Features\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.09595959595959595\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['work', 'great', 'good', 'company', 'people', 'benefit', 'pay', 'opportunity', 'balance', 'life', 'employee', 'culture', 'time', 'management', 'job', 'salary', 'environment', 'none', 'lot', 'hour']\n",
      "----\n",
      "\n",
      "Q3 Unigram Reviews with 5000 Features\n",
      "============\n",
      "Done with 500 reviews\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  1.0 \n",
      " The validation accuracy is:  0.09595959595959595\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     pos   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "TOP TEN IMPORTANT FEATURES:\n",
      "['great', 'good', 'work', 'company', 'pay', 'people', 'benefit', 'management', 'employee', 'culture', 'life', 'opportunity', 'job', 'balance', 'time', 'schedule', 'lot', 'salary', 'none', 'hour']\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q1 - Unigram (ngram = 1)\n",
    "print(\"Q1 Unigram Lemmatized Reviews\\n============\")\n",
    "new_clean_reviews = review_cleaner(train[\"review\"], lemmatize = True, stem = False)\n",
    "train_predict_sentiment(cleaned_reviews = new_clean_reviews, y = train[\"sentiment\"], ngram = 1, max_features = 100)\n",
    "print(\"----\\n\")\n",
    "\n",
    "print(\"Q1 Unigram Stemmed Reviews\\n============\")\n",
    "new_clean_reviews = review_cleaner(train[\"review\"], lemmatize = False, stem = True)\n",
    "train_predict_sentiment(cleaned_reviews = new_clean_reviews, y = train[\"sentiment\"], ngram = 1, max_features = 100)\n",
    "print(\"----\\n\")\n",
    "\n",
    "# Q2 - Bigram (ngram = 2)\n",
    "print(\"Q2 Bigram Lemmatized Reviews\\n============\")\n",
    "new_clean_reviews = review_cleaner(train[\"review\"], lemmatize = True, stem = False)\n",
    "train_predict_sentiment(cleaned_reviews = new_clean_reviews, y = train[\"sentiment\"], ngram = 2, max_features = 100)\n",
    "print(\"----\\n\")\n",
    "\n",
    "print(\"Q2 Bigram Stemmed Reviews\\n============\")\n",
    "new_clean_reviews = review_cleaner(train[\"review\"], lemmatize = False, stem = True)\n",
    "train_predict_sentiment(cleaned_reviews = new_clean_reviews, y = train[\"sentiment\"], ngram = 2, max_features = 100)\n",
    "print(\"----\\n\")\n",
    "\n",
    "# Q3 - Multiple Unigrams\n",
    "for i in [10, 100, 1000, 5000]:\n",
    "    print(f\"Q3 Unigram Reviews with {i} Features\\n============\")\n",
    "    new_clean_reviews = review_cleaner(train[\"review\"], lemmatize = True, stem = False)\n",
    "    train_predict_sentiment(cleaned_reviews = new_clean_reviews, y = train[\"sentiment\"], ngram = 1, max_features = i)\n",
    "    print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
